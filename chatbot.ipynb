{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0113b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (1.0.20)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: gemma in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (4.14.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (0.16.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (0.21.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (2.11.7)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (3.11.2)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (14.1.0)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (2.2.6)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (0.35.0)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from gemma) (0.6)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: certifi in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
      "Requirement already satisfied: idna in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: anyio in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: requests in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: sympy in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (6.32.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb pymupdf pillow gemma \n",
    "#pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf4db72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (0.5.3)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from ollama) (2.11.7)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
      "Requirement already satisfied: idna in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea3c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDF Conversion to Handle Multiple PDFs\n",
    "import os\n",
    "\n",
    "def convert_pdfs_in_folder(folder_path, output_dir):\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    \n",
    "    all_image_paths = []\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        # Convert each PDF to images\n",
    "        image_paths = convert_pdf_to_images(pdf_path, output_dir)\n",
    "        all_image_paths.extend(image_paths)\n",
    "    \n",
    "    return all_image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5adf84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert PDF Pages to Images\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def convert_pdf_to_images(pdf_path, output_dir):\n",
    "    # Open the provided PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Iterate over each page and convert to image\n",
    "    image_paths = []\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        # Get the page\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        \n",
    "        # Convert page to pixmap (image)\n",
    "        pix = page.get_pixmap()\n",
    "        \n",
    "        # Convert to PIL Image for easier manipulation\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        \n",
    "        # Create an image file path\n",
    "        img_path = os.path.join(output_dir, f\"page_{page_num+1}.png\")\n",
    "        img.save(img_path)\n",
    "        image_paths.append(img_path)\n",
    "    \n",
    "    return image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify OCR to Handle Multiple PDFs\n",
    "def extract_text_from_all_pdfs(folder_path, output_dir):\n",
    "    # Step 1: Convert all PDFs to images\n",
    "    image_paths = convert_pdfs_in_folder(folder_path, output_dir)\n",
    "    \n",
    "    # Step 2: Extract text using OCR (Gemma)\n",
    "    extracted_texts = extract_text_from_images(image_paths)\n",
    "    \n",
    "    return extracted_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a8e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply OCR using Gemma\n",
    "# from gemma import Gemma\n",
    "\n",
    "# def extract_text_from_images(image_paths):\n",
    "#     # Initialize Gemma OCR model\n",
    "#     gemma_model = Gemma()\n",
    "\n",
    "#     # Extract text from each image\n",
    "#     extracted_texts = []\n",
    "#     for img_path in image_paths:\n",
    "#         text = gemma_model.extract_text(img_path)\n",
    "#         extracted_texts.append(text)\n",
    "    \n",
    "#     return extracted_texts\n",
    "\n",
    "\n",
    "\n",
    "#from ollama import Ollama\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import ollama\n",
    "\n",
    "def image_to_base64(img_path):\n",
    "    \"\"\"Convert image to base64 string\"\"\"\n",
    "    with open(img_path, \"rb\") as f:\n",
    "        encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    return encoded\n",
    "\n",
    "def extract_text_from_images_with_gemma(image_paths):\n",
    "    \"\"\"\n",
    "    Use Gemma 3:12B via Ollama to extract text from images.\n",
    "    \"\"\"\n",
    "    extracted_texts = []\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        img_b64 = image_to_base64(img_path)\n",
    "        \n",
    "        # Prepare prompt for OCR\n",
    "        prompt = f\"Extract all text from this image and return as plain text:\\n\\n[image_data_base64:{img_b64}]\"\n",
    "        \n",
    "        # Call Ollama Gemma model \n",
    "        response = ollama.chat(model=\"gemma3:12b\", messages=[{\"role\":\"user\", \"content\": prompt}])\n",
    "        \n",
    "        extracted_text = response[\"message\"][\"content\"]\n",
    "        extracted_texts.append(extracted_text)\n",
    "    \n",
    "    return extracted_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68cc9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Extracted Text to .txt Files\n",
    "# def save_extracted_text_as_txt(extracted_texts, output_dir):\n",
    "#     \"\"\"Save the extracted text from images into .txt files in a new folder.\"\"\"\n",
    "#     text_output_dir = os.path.join(output_dir, \"extracted_texts\")\n",
    "#     if not os.path.exists(text_output_dir):\n",
    "#         os.makedirs(text_output_dir)\n",
    "    \n",
    "#     for i, text in enumerate(extracted_texts):\n",
    "#         file_path = os.path.join(text_output_dir, f\"page_{i+1}.txt\")\n",
    "#         with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(text)\n",
    "#     print(f\"Extracted texts have been saved to {text_output_dir}\")\n",
    "\n",
    "# Function to save the extracted text as txt files\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Function to save the extracted text as txt files and display it\n",
    "def save_extracted_text_as_txt(extracted_texts, output_dir):\n",
    "    \"\"\"Save the extracted text from images into .txt files in a new folder and display the text.\"\"\"\n",
    "    \n",
    "    # Create directory for storing text files\n",
    "    text_output_dir = os.path.join(output_dir, \"extracted_texts\")\n",
    "    if not os.path.exists(text_output_dir):\n",
    "        os.makedirs(text_output_dir)\n",
    "    \n",
    "    # Loop through each extracted text, save to file, and display it\n",
    "    for i, text in enumerate(extracted_texts):\n",
    "        file_path = os.path.join(text_output_dir, f\"page_{i+1}.txt\")\n",
    "        \n",
    "        # Save the text to the file\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        \n",
    "        # Display the text in the notebook output immediately after saving\n",
    "        display(Markdown(f\"### Extracted Text from Page {i+1}:\"))\n",
    "        display(Markdown(f\"```\\n{text}\\n```\"))\n",
    "\n",
    "    print(f\"Extracted texts have been saved to {text_output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798126a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChromaDB Storage to Include Multiple PDFs\n",
    "def add_texts_to_chromadb_for_multiple_pdfs(extracted_texts, folder_path):\n",
    "    # Get the ChromaDB collection\n",
    "    collection = client.get_collection(name=\"pdf_knowledgebase\")\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    all_metadata = []\n",
    "    \n",
    "    # Step 1: Add each extracted text from multiple PDFs to ChromaDB\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        extracted_text = extracted_texts.get(pdf_file, [])\n",
    "        \n",
    "        for page_num, text in enumerate(extracted_text):\n",
    "            # Create metadata for each page\n",
    "            metadata = {'pdf_name': pdf_file, 'page': page_num + 1}\n",
    "            collection.add(\n",
    "                documents=[text],  # Add the text of the page\n",
    "                metadatas=[metadata],  # Add metadata\n",
    "                ids=[f\"{pdf_file}_page_{page_num + 1}\"],  # Unique ID for each page\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc1fb63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (1.0.20)\n",
      "Requirement already satisfied: chroma-migrate in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (3.11.2)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (0.21.4)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (2.11.7)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (0.16.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (2.2.6)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (0.35.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (14.1.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chromadb) (4.14.1)\n",
      "Requirement already satisfied: duckdb==0.7.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chroma-migrate) (0.7.1)\n",
      "Requirement already satisfied: more-itertools>=9.1.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chroma-migrate) (10.7.0)\n",
      "Requirement already satisfied: chroma-bullet in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chroma-migrate) (2.2.0)\n",
      "Requirement already satisfied: clickhouse-connect==0.6.6 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from chroma-migrate) (0.6.6)\n",
      "Requirement already satisfied: urllib3>=1.26 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from clickhouse-connect==0.6.6->chroma-migrate) (2.5.0)\n",
      "Requirement already satisfied: zstandard in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from clickhouse-connect==0.6.6->chroma-migrate) (0.24.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from clickhouse-connect==0.6.6->chroma-migrate) (8.7.0)\n",
      "Requirement already satisfied: lz4 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from clickhouse-connect==0.6.6->chroma-migrate) (4.4.4)\n",
      "Requirement already satisfied: pytz in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from clickhouse-connect==0.6.6->chroma-migrate) (2025.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from clickhouse-connect==0.6.6->chroma-migrate) (2025.8.3)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: requests in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (6.32.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from importlib-metadata->clickhouse-connect==0.6.6->chroma-migrate) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\busra\\melikoz\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade chromadb chroma-migrate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6322fadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Busra\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|██████████| 79.3M/79.3M [17:24<00:00, 79.7kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id2', 'id1']], 'embeddings': None, 'documents': [['This is a document about oranges', 'This is a document about pineapple']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[None, None]], 'distances': [[1.1462137699127197, 1.3015384674072266]]}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize a local Chroma client (database stored in ./chroma_db)\n",
    "# client = chromadb.Client(\n",
    "#     persist_directory=\"./chroma_db\",  # folder where data is stored\n",
    "#     chroma_db_impl=\"duckdb+parquet\"   # local database backend\n",
    "# )\n",
    "\n",
    "\n",
    "#import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# switch `create_collection` to `get_or_create_collection` to avoid creating a new collection every time\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")\n",
    "\n",
    "# switch `add` to `upsert` to avoid adding the same documents every time\n",
    "collection.upsert(\n",
    "    documents=[\n",
    "        \"This is a document about pineapple\",\n",
    "        \"This is a document about oranges\"\n",
    "    ],\n",
    "    ids=[\"id1\", \"id2\"]\n",
    ")\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"This is a query document about florida\"], # Chroma will embed this for you\n",
    "    n_results=2 # how many results to return\n",
    ")\n",
    "\n",
    "print(results)\n",
    "\n",
    "# # Create a collection (like a table)\n",
    "# try:\n",
    "#     collection = chroma_client.get_or_create_collection(name=\"my_collection\")\n",
    "#     #collection = client.get_collection(\"pdf_knowledgebase\")\n",
    "# except ValueError:\n",
    "#     collection = client.create_collection(\"pdf_knowledgebase\")\n",
    "\n",
    "# # Add documents\n",
    "# texts = [\"This is page 1 text\", \"This is page 2 text\"]\n",
    "# metadata = [{\"pdf_name\": \"example.pdf\", \"page\": 1}, {\"pdf_name\": \"example.pdf\", \"page\": 2}]\n",
    "# ids = [\"example_1\", \"example_2\"]\n",
    "\n",
    "# collection.add(documents=texts, metadatas=metadata, ids=ids)\n",
    "\n",
    "# # Query: Find similar text\n",
    "# results = collection.query(\n",
    "#     query_texts=[\"page 1\"],\n",
    "#     n_results=1\n",
    "# )\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a78f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ChromaDB Storage to Include pdfs\n",
    "\n",
    "import chromadb\n",
    "\n",
    "# Initialize Chroma client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Get or create a collection (avoiding creating a new one every time)\n",
    "collection = chroma_client.get_or_create_collection(name=\"pdf_knowledgebase\")\n",
    "\n",
    "def add_text_to_chromadb(extracted_texts, metadata_list):\n",
    "    \"\"\"\n",
    "    Add or update extracted text documents in ChromaDB.\n",
    "    \"\"\"\n",
    "    # Prepare data for upserting\n",
    "    ids = [f\"{meta['pdf_name']}_page_{meta['page']}\" for meta in metadata_list]\n",
    "    \n",
    "    # Upsert documents and their metadata\n",
    "    collection.upsert(\n",
    "        documents=extracted_texts,  # list of extracted text\n",
    "        ids=ids,                    # unique IDs based on PDF name and page number\n",
    "        metadatas=metadata_list     # metadata for each document\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188472a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the Chatbot (GPT-OSS 20B)\n",
    "# from ollama import Ollama\n",
    "\n",
    "# def gpt_oss_20b_query(query, collection):\n",
    "#     # Retrieve the most relevant documents from ChromaDB\n",
    "#     results = collection.query(query, n_results=5)  # fetch top 5 results\n",
    "\n",
    "#     # Combine the retrieved documents' texts\n",
    "#     context = \" \".join([result['document'] for result in results['documents']])\n",
    "\n",
    "#     # Send the query to the GPT-OSS model with context\n",
    "#     model = Ollama(model=\"gpt-oss:20b\")\n",
    "#     response = model.chat(context + \"\\n\" + query)\n",
    "    \n",
    "#     return response['text']\n",
    "\n",
    "\n",
    "\n",
    "# import ollama\n",
    "\n",
    "# def gpt_oss_20b_query(query, collection):\n",
    "#     # Retrieve the most relevant documents from ChromaDB\n",
    "#     results = collection.query(query, n_results=5)  # fetch top 5 results\n",
    "\n",
    "#     # Combine the retrieved documents' texts\n",
    "#     context = \" \".join([result['document'] for result in results['documents']])\n",
    "\n",
    "#     # Prepare the prompt for the model: Combining context and user query\n",
    "#     prompt = context + \"\\n\\nUser Query: \" + query\n",
    "\n",
    "#     # Call the local GPT-OSS 20B model via Ollama\n",
    "#     response = ollama.chat(model=\"gpt-oss:20b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "#     # Return the model's response\n",
    "#     return response[\"message\"][\"content\"]\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# import ollama\n",
    "\n",
    "# def gpt_oss_20b_query(query, collection):\n",
    "#     # Retrieve the most relevant documents from ChromaDB\n",
    "#     results = collection.query(query, n_results=5)  # fetch top 5 results\n",
    "\n",
    "#     # Correct extraction of documents from the results\n",
    "#     # The results['documents'] is a list of lists, so extract the first list\n",
    "#     documents = results['documents'][0]  # The first element contains the documents\n",
    "\n",
    "#     # Combine the retrieved documents' texts into context\n",
    "#     context = \" \".join(documents)\n",
    "\n",
    "#     # Prepare the prompt for the model: Combining context and user query\n",
    "#     prompt = context + \"\\n\\nUser Query: \" + query\n",
    "\n",
    "#     # Call the local GPT-OSS 20B model via Ollama\n",
    "#     response = ollama.chat(model=\"gpt-oss:20b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "#     # Return the model's response\n",
    "#     return response[\"message\"][\"content\"]\n",
    "\n",
    "import ollama\n",
    "\n",
    "def gpt_oss_20b_query(query, collection):\n",
    "    # Retrieve the most relevant documents from ChromaDB based on the user's query\n",
    "    results = collection.query(\n",
    "        query_texts=[query],  # Pass query text directly\n",
    "        n_results=5  # Fetch top 5 results\n",
    "    )\n",
    "\n",
    "    # Correct extraction of documents from the query results\n",
    "    # The results['documents'] is a list of lists, so extract the first list\n",
    "    documents = results['documents'][0]  # The first element contains the documents\n",
    "\n",
    "    # Combine the retrieved documents' texts into context\n",
    "    context = \" \".join(documents)\n",
    "\n",
    "    # Prepare the prompt for the model: Combining context and user query\n",
    "    prompt = context + \"\\n\\nUser Query: \" + query\n",
    "\n",
    "    # Call the local GPT-OSS 20B model via Ollama with the combined context and user query\n",
    "    response = ollama.chat(model=\"gpt-oss:20b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "    # Return the model's response\n",
    "    return response[\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dae55d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle Deletion of PDFs and Associated Data\n",
    "def remove_pdf_from_chromadb(pdf_name):\n",
    "    # Get the collection\n",
    "    collection = client.get_collection(name=\"pdf_knowledgebase\")\n",
    "    \n",
    "    # Query to find all pages related to the PDF\n",
    "    results = collection.query(f\"{pdf_name}\", n_results=100)\n",
    "    \n",
    "    # Remove the documents that belong to this PDF\n",
    "    for result in results['documents']:\n",
    "        collection.delete(ids=[result['id']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea84558",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Handle Deletion of PDFs and Associated Data with chromadb\n",
    "def remove_pdf_from_chromadb(pdf_name):\n",
    "    \"\"\"Remove all pages related to the specified PDF from ChromaDB.\"\"\"\n",
    "    # Get the collection\n",
    "    chroma_client = chromadb.Client()  # Create a client if not already done\n",
    "    collection = chroma_client.get_or_create_collection(name=\"pdf_knowledgebase\")\n",
    "    \n",
    "    # Query to find all pages related to the PDF (based on the pdf_name in metadata)\n",
    "    results = collection.query(query_texts=[pdf_name], n_results=100)\n",
    "    \n",
    "    # Remove the documents that belong to this PDF\n",
    "    for result in results['documents']:\n",
    "        # Delete each document by its unique ID\n",
    "        collection.delete(ids=[result['id']])\n",
    "    \n",
    "    print(f\"All documents related to {pdf_name} have been removed from the collection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14251001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the extracted texts\n",
    "\n",
    "\n",
    "# Path for PDF and Output directory\n",
    "pdf_path = \"./PDF/return 2 pages.pdf\"  \n",
    "output_dir = \"output\"   \n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Step 1: Convert PDF to Images\n",
    "image_paths = convert_pdf_to_images(pdf_path, output_dir)\n",
    "\n",
    "# Step 2: Apply OCR to Extract Text\n",
    "extracted_texts = extract_text_from_images_with_gemma(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c567a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Extracted Text from Page 1:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "This is a base64 encoded image. Decoding it reveals an image of a stylized, colorful, and possibly abstract landscape. Here's a breakdown of what it likely depicts and a summary:\n",
       "\n",
       "**Observations:**\n",
       "\n",
       "*   **Landforms:** It seems to feature rolling hills, a suggestion of a river or lake, and possibly mountains or rock formations in the background. The landforms are highly stylized, not realistic.\n",
       "*   **Colors:** A broad palette is used, mixing blues, greens, yellows, oranges, and purples. This suggests a dreamlike or fantastical atmosphere.\n",
       "*   **Style:** The artwork has a graphic, almost digital look. It could be a vector illustration or have been digitally painted. The shapes are simplified and often blend into one another.\n",
       "*   **Possible Theme:** The overall feeling is one of tranquility and natural beauty, but the abstract nature invites personal interpretation. It could be a scene from a fictional world or a symbolic representation of nature.\n",
       "\n",
       "\n",
       "\n",
       "**Summary:**\n",
       "The base64 string represents a colorful, abstract landscape image, likely intended to evoke a sense of peace and wonder.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Extracted Text from Page 2:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "This appears to be a binary blob, likely an image encoded as Base64.  Decoding it will reveal its true form, but without decoding it, it's just a string of characters.\n",
       "\n",
       "Here's the likely interpretation:\n",
       "\n",
       "*   **It's a Base64 Encoded Image:** The string is structured in a way that's characteristic of Base64 encoding. This means it represents image data that has been converted into a text string for safe transmission or storage.\n",
       "*   **Decoding Required:** To see what the image *is*, you need to decode it. Online Base64 decoders (easily found with a search) can do this.  Common image formats like PNG or JPG are likely.\n",
       "\n",
       "**To see the image, I need to decode it.** I cannot directly perform actions like decoding images in this text-based environment. Use a Base64 decoder to view the actual image.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted texts have been saved to text_putput\\extracted_texts\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Store Text in ChromaDB\n",
    "# metadata = [{'pdf_name': pdf_path, 'page': i+1} for i in range(len(extracted_texts))]\n",
    "# add_text_to_chromadb(extracted_texts, metadata) \n",
    "\n",
    "# Step 4: Save extracted text to .txt files\n",
    "\n",
    "output_dir = \"text_putput\"  # Directory to save the text files\n",
    "\n",
    "save_extracted_text_as_txt(extracted_texts, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20ae7a97",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"gpt-oss-20b\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 227\u001b[0m\n\u001b[0;32m    224\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(documents)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# Now you can use the context to generate the response\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgpt_oss_20b_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass the collection instead of context\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[39], line 80\u001b[0m, in \u001b[0;36mgpt_oss_20b_query\u001b[1;34m(query, collection)\u001b[0m\n\u001b[0;32m     77\u001b[0m prompt \u001b[38;5;241m=\u001b[39m context \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUser Query: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m query\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Call the local GPT-OSS 20B model via Ollama with the combined context and user query\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-oss-20b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Return the model's response\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Busra\\Melikoz\\venv\\lib\\site-packages\\ollama\\_client.py:342\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    298\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    299\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    309\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Busra\\Melikoz\\venv\\lib\\site-packages\\ollama\\_client.py:180\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    178\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\Busra\\Melikoz\\venv\\lib\\site-packages\\ollama\\_client.py:124\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[0;32m    126\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mResponseError\u001b[0m: model \"gpt-oss-20b\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "# pdf_path = \"./PDF\"\n",
    "# output_dir = \"output\"\n",
    "\n",
    "# # Step 1: Convert PDF to Images\n",
    "# image_paths = convert_pdf_to_images(pdf_path, output_dir)\n",
    "\n",
    "# # Step 2: Apply OCR to Extract Text\n",
    "# extracted_texts = extract_text_from_images_with_gemma(image_paths)\n",
    "\n",
    "# # Step 3: Store Text in ChromaDB\n",
    "# metadata = [{'pdf_name': pdf_path, 'page': i+1} for i in range(len(extracted_texts))]\n",
    "# add_text_to_chromadb(extracted_texts, metadata)\n",
    "\n",
    "# # Step 4: Use Chatbot for Querying\n",
    "# user_query = \"What is the service about?\"\n",
    "# response = gpt_oss_20b_query(user_query, client.get_collection(name=\"pdf_knowledgebase\"))\n",
    "# print(response)\n",
    "\n",
    "#------------------------------\n",
    "# Example Usage\n",
    "# pdf_path = \"./PDF/Return Policy.pdf\"  # Path to the folder with PDFs\n",
    "# output_dir = \"output\"  # Directory where images will be saved\n",
    "\n",
    "# # Step 1: Convert PDF to Images\n",
    "# image_paths = convert_pdf_to_images(pdf_path, output_dir)\n",
    "\n",
    "# # Step 2: Apply OCR to Extract Text\n",
    "# extracted_texts = extract_text_from_images_with_gemma(image_paths)\n",
    "\n",
    "# # Step 3: Store Text in ChromaDB\n",
    "# metadata = [{'pdf_name': pdf_path, 'page': i+1} for i in range(len(extracted_texts))]\n",
    "# add_text_to_chromadb(extracted_texts, metadata)\n",
    "\n",
    "# # Step 4: Use Chatbot for Querying\n",
    "# user_query = \"What is the service about?\"\n",
    "# chroma_client = chromadb.Client()\n",
    "# collection = chroma_client.get_or_create_collection(name=\"pdf_knowledgebase\")\n",
    "# response = gpt_oss_20b_query(user_query, collection)\n",
    "# print(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------\n",
    "# import os\n",
    "\n",
    "# # Example Usage\n",
    "# pdf_path = \"./PDF/Return Policy.pdf\"  # Path to the PDF\n",
    "# output_dir = \"output\"  # Directory where images will be saved\n",
    "\n",
    "# # Ensure the output directory exists\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Step 1: Convert PDF to Images\n",
    "# image_paths = convert_pdf_to_images(pdf_path, output_dir)\n",
    "\n",
    "# # Step 2: Apply OCR to Extract Text\n",
    "# extracted_texts = extract_text_from_images_with_gemma(image_paths)\n",
    "\n",
    "# # Step 3: Store Text in ChromaDB\n",
    "# metadata = [{'pdf_name': pdf_path, 'page': i+1} for i in range(len(extracted_texts))]\n",
    "# add_text_to_chromadb(extracted_texts, metadata)\n",
    "\n",
    "# # Step 4: Use Chatbot for Querying\n",
    "# user_query = \"What is the service about?\"\n",
    "# chroma_client = chromadb.Client()\n",
    "# collection = chroma_client.get_or_create_collection(name=\"pdf_knowledgebase\")\n",
    "# response = gpt_oss_20b_query(user_query, collection)\n",
    "# print(response)\n",
    "\n",
    "#------------------------------------------------\n",
    "#------------------------------------------------\n",
    "# import chromadb\n",
    "# import numpy as np\n",
    "\n",
    "# # Example Usage \n",
    "# pdf_path = \"./PDF/return 2 pages.pdf\"  # Path to the PDF\n",
    "# output_dir = \"output\"  # Directory where images will be saved\n",
    "\n",
    "# # Ensure the output directory exists\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Step 1: Convert PDF to Images\n",
    "# image_paths = convert_pdf_to_images(pdf_path, output_dir)\n",
    "\n",
    "# # Step 2: Apply OCR to Extract Text\n",
    "# extracted_texts = extract_text_from_images_with_gemma(image_paths)\n",
    "\n",
    "# # Step 3: Store Text in ChromaDB\n",
    "# metadata = [{'pdf_name': pdf_path, 'page': i+1} for i in range(len(extracted_texts))]\n",
    "# add_text_to_chromadb(extracted_texts, metadata)\n",
    "\n",
    "# # Step 4: Use Chatbot for Querying\n",
    "\n",
    "# # Query Text (no need to embed manually if Chroma handles it internally)\n",
    "# user_query = \"What is the service about?\"\n",
    "\n",
    "# # Chroma Client\n",
    "# chroma_client = chromadb.Client()\n",
    "# collection = chroma_client.get_or_create_collection(name=\"pdf_knowledgebase\")\n",
    "\n",
    "# # Get embeddings of the user query using Chroma's internal embedding model\n",
    "# #query_embeddings = collection.get_embeddings([user_query])\n",
    "\n",
    "\n",
    "# # Perform the query with ChromaDB (querying the text directly, Chroma will embed it internally)\n",
    "# results = collection.query(\n",
    "#     query_texts=[user_query],  # Pass query text directly\n",
    "#     n_results=5  # Fetch top 5 results\n",
    "# )\n",
    "\n",
    "# # Perform the query with embeddings\n",
    "# # results = collection.query(\n",
    "# #     query_embeddings=query_embeddings,  # Use query embeddings here\n",
    "# #     n_results=5  # Fetch top 5 results\n",
    "# # )\n",
    "\n",
    "# # Combine the retrieved documents' texts into context\n",
    "# context = \" \".join([result['document'] for result in results['documents']])\n",
    "\n",
    "# # Generate the final response with GPT-OSS 20B model using the context\n",
    "# response = gpt_oss_20b_query(user_query, collection)\n",
    "# print(response)\n",
    "#------------------------------------------------\n",
    "#------------------------------------------------\n",
    "#------------------------------------------------\n",
    "# import chromadb\n",
    "# import os\n",
    "\n",
    "# # Example Usage\n",
    "# pdf_path = \"./PDF/return 2 pages.pdf\"  # Path to the PDF\n",
    "# output_dir = \"output\"  # Directory where images will be saved\n",
    "\n",
    "# # Ensure the output directory exists\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Step 1: Convert PDF to Images (Assuming convert_pdf_to_images function is defined)\n",
    "# image_paths = convert_pdf_to_images(pdf_path, output_dir)\n",
    "\n",
    "# # Step 2: Apply OCR to Extract Text (Assuming extract_text_from_images_with_gemma function is defined)\n",
    "# extracted_texts = extract_text_from_images_with_gemma(image_paths)\n",
    "\n",
    "# # Step 3: Store Text in ChromaDB\n",
    "# metadata = [{'pdf_name': pdf_path, 'page': i+1} for i in range(len(extracted_texts))]\n",
    "# add_text_to_chromadb(extracted_texts, metadata)\n",
    "\n",
    "# # Step 4: Use Chatbot for Querying\n",
    "\n",
    "# # Query Text (no need to embed manually if Chroma handles it internally)\n",
    "# user_query = \"What is the service about?\"\n",
    "\n",
    "# # Chroma Client\n",
    "# chroma_client = chromadb.Client()\n",
    "# collection = chroma_client.get_or_create_collection(name=\"pdf_knowledgebase\")\n",
    "\n",
    "# # Get embeddings of the user query using Chroma's internal embedding model\n",
    "# results = collection.query(\n",
    "#     query_texts=[user_query],  # Pass query text directly\n",
    "#     n_results=5  # Fetch top 5 results\n",
    "# )\n",
    "\n",
    "# # Correct the extraction of the documents from the query results\n",
    "# # The results['documents'] is a list of lists, so we need to extract the first element\n",
    "# documents = results['documents'][0]  # The first element contains the documents\n",
    "\n",
    "# # Join the retrieved documents' texts into context\n",
    "# context = \" \".join(documents)\n",
    "\n",
    "# # Now you can use the context to generate the response\n",
    "# response = gpt_oss_20b_query(user_query, context)  # Assuming gpt_oss_20b_query is defined elsewhere\n",
    "\n",
    "# print(response)\n",
    "\n",
    "#------------------------------------------------\n",
    "#------------------------------------------------\n",
    "#------------------------------------------------\n",
    "#------------------------------------------------\n",
    "#------------------------------------------------\n",
    "\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = \"./PDF/return 2 pages.pdf\"  # Path to the PDF\n",
    "output_dir = \"output\"  # Directory where images will be saved\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Step 1: Convert PDF to Images (Assuming convert_pdf_to_images function is defined)\n",
    "image_paths = convert_pdf_to_images(pdf_path, output_dir)\n",
    "\n",
    "# Step 2: Apply OCR to Extract Text (Assuming extract_text_from_images_with_gemma function is defined)\n",
    "extracted_texts = extract_text_from_images_with_gemma(image_paths)\n",
    "\n",
    "# Step 3: Store Text in ChromaDB\n",
    "metadata = [{'pdf_name': pdf_path, 'page': i+1} for i in range(len(extracted_texts))]\n",
    "add_text_to_chromadb(extracted_texts, metadata)\n",
    "\n",
    "# Step 4: Use Chatbot for Querying\n",
    "\n",
    "# Query Text (no need to embed manually if Chroma handles it internally)\n",
    "user_query = \"What is the service about?\"\n",
    "\n",
    "# Chroma Client\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(name=\"pdf_knowledgebase\")\n",
    "\n",
    "# Get embeddings of the user query using Chroma's internal embedding model\n",
    "results = collection.query(\n",
    "    query_texts=[user_query],  # Pass query text directly\n",
    "    n_results=5  # Fetch top 5 results\n",
    ")\n",
    "\n",
    "# Correct the extraction of the documents from the query results\n",
    "# The results['documents'] is a list of lists, so we need to extract the first element\n",
    "documents = results['documents'][0]  # The first element contains the documents\n",
    "\n",
    "# Join the retrieved documents' texts into context\n",
    "context = \" \".join(documents)\n",
    "\n",
    "# Now you can use the context to generate the response\n",
    "response = gpt_oss_20b_query(user_query, collection)  # Pass the collection instead of context\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all\n",
    "folder_path = \"./PDF\"  # Folder containing PDFs\n",
    "output_dir = \"output_images\"  # Folder to store images\n",
    "\n",
    "# Step 1: Extract text from all PDFs\n",
    "extracted_texts = extract_text_from_all_pdfs(folder_path, output_dir)\n",
    "\n",
    "# Create a dictionary to organize extracted text by PDF\n",
    "organized_texts = {}\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "for pdf_file in pdf_files:\n",
    "    extracted_text = extracted_texts  # Extract text for this PDF (adjust as needed)\n",
    "    organized_texts[pdf_file] = extracted_text\n",
    "\n",
    "# Step 2: Add the extracted texts to ChromaDB\n",
    "add_texts_to_chromadb_for_multiple_pdfs(organized_texts, folder_path)\n",
    "\n",
    "# Step 3: Querying the chatbot\n",
    "user_query = \"What is the service about?\"\n",
    "response = gpt_oss_20b_query(user_query, client.get_collection(name=\"pdf_knowledgebase\"))\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
